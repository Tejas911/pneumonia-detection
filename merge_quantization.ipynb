{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load the two models\n",
    "model1 = tf.keras.models.load_model('xception_train1.h5')\n",
    "model2 = tf.keras.models.load_model('xception_train2.h5')\n",
    "model3 = tf.keras.models.load_model('xception_train3.h5')\n",
    "model4 = tf.keras.models.load_model('xception_train4.h5')\n",
    "\n",
    "# Ensure that both models have the same architecture\n",
    "assert model1.count_params() == model2.count_params(), \"Models do not have the same architecture!\"\n",
    "\n",
    "# Extract weights from both models\n",
    "weights1 = model1.get_weights()\n",
    "weights2 = model2.get_weights()\n",
    "weights3 = model2.get_weights()\n",
    "weights4 = model2.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 weights: 238\n",
      "Model 2 weights: 238\n",
      "Model 3 weights: 238\n",
      "Model 4 weights: 238\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model 1 weights: {len(weights1)}\")\n",
    "print(f\"Model 2 weights: {len(weights2)}\")\n",
    "print(f\"Model 3 weights: {len(weights3)}\")\n",
    "print(f\"Model 4 weights: {len(weights4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Ensure that both models have the same architecture\n",
    "assert model1.count_params() == model2.count_params(), \"Models do not have the same architecture!\"\n",
    "\n",
    "# Extract weights from both models\n",
    "weights1 = model1.get_weights()\n",
    "weights2 = model2.get_weights()\n",
    "weights3 = model3.get_weights()\n",
    "weights4 = model4.get_weights()\n",
    "\n",
    "# Initialize a new model by copying the architecture and weights from model1\n",
    "merged_model = tf.keras.models.clone_model(model1)\n",
    "merged_model.set_weights(weights1)\n",
    "\n",
    "# Accuracies of the models\n",
    "acc1, acc2, acc3, acc4 = 0.66, 0.87, 0.82, 0.83\n",
    "# Normalize the accuracies to get weights\n",
    "total_acc = acc1 + acc2 + acc3 + acc4\n",
    "wx1 = acc1 / total_acc\n",
    "wx2 = acc2 / total_acc\n",
    "wx3 = acc3 / total_acc\n",
    "wx4 = acc4 / total_acc\n",
    "\n",
    "# n = 0.05 +0.6+0.15+0.15 # Experimental value\n",
    "# n = 0.795\n",
    "n = 0.97\n",
    "\n",
    "\n",
    "average_weights = [((w1*wx1) + (w2*wx2) + (w3*wx3) + (w4*wx4)) / n for w1, w2, w3, w4 in zip(weights1, weights2, weights3, weights4)]\n",
    "\n",
    "merged_model.set_weights(average_weights)\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save('merged_model.h5')\n",
    "\n",
    "print(\"Merged model saved successfully.\\nQuantized model saved successfully!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the dataset for testing\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    r\"split_chest_xray\\test\",  # Replace with the path to your testing dataset directory\n",
    "    target_size=(224, 224),\n",
    "    # batch_size=32,\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model3 = tf.keras.models.load_model('merged_model.h5')\n",
    "model3.compile(optimizer='adam',  # or your preferred optimizer\n",
    "               loss='categorical_crossentropy',  # or your preferred loss function\n",
    "               metrics=['accuracy'])  # or your preferred metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 3s/step - accuracy: 0.8784 - loss: 0.3817\n",
      "Test Loss: 0.3796602189540863, Test Accuracy: 0.8894230723381042\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the testing data\n",
    "test_loss, test_accuracy = model3.evaluate(test_generator)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_size(file_path):\n",
    "    # Get the file size in bytes\n",
    "    file_size_bytes = os.path.getsize(file_path)\n",
    "    \n",
    "    # Convert bytes to megabytes (1 MB = 1024 * 1024 bytes)\n",
    "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "    \n",
    "    return f\"The size of the file '{file_path}' is: {file_size_mb:.2f} MB\\n\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the file 'xception_train_entire_dataset.h5' is: 262.94 MB\n",
      "\n",
      "The size of the file 'xception_train1.h5' is: 262.94 MB\n",
      "\n",
      "The size of the file 'xception_train2.h5' is: 262.94 MB\n",
      "\n",
      "The size of the file 'xception_train3.h5' is: 262.94 MB\n",
      "\n",
      "The size of the file 'xception_train3.h5' is: 262.94 MB\n",
      "\n",
      "\n",
      "\n",
      "New MERGED FILE: The size of the file 'merged_model.h5' is: 88.02 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_size(r\"xception_train_entire_dataset.h5\"))\n",
    "print(get_size(r\"xception_train1.h5\"))\n",
    "print(get_size(r\"xception_train2.h5\"))\n",
    "print(get_size(r\"xception_train3.h5\"))\n",
    "print(get_size(r\"xception_train3.h5\"))\n",
    "\n",
    "print(\"\\n\\nNew MERGED FILE:\",get_size(r\"merged_model.h5\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load the two models\n",
    "model1 = tf.keras.models.load_model('xception_train1.h5')\n",
    "model2 = tf.keras.models.load_model('xception_train2.h5')\n",
    "model3 = tf.keras.models.load_model('xception_train3.h5')\n",
    "model4 = tf.keras.models.load_model('xception_train4.h5')\n",
    "\n",
    "# Ensure that both models have the same architecture\n",
    "assert model1.count_params() == model2.count_params(), \"Models do not have the same architecture!\"\n",
    "\n",
    "# Extract weights from both models\n",
    "weights1 = model1.get_weights()\n",
    "weights2 = model2.get_weights()\n",
    "weights3 = model2.get_weights()\n",
    "weights4 = model2.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 weights: 238\n",
      "Model 2 weights: 238\n",
      "Model 3 weights: 238\n",
      "Model 4 weights: 238\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model 1 weights: {len(weights1)}\")\n",
    "print(f\"Model 2 weights: {len(weights2)}\")\n",
    "print(f\"Model 3 weights: {len(weights3)}\")\n",
    "print(f\"Model 4 weights: {len(weights4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Ensure that both models have the same architecture\n",
    "assert model1.count_params() == model2.count_params(), \"Models do not have the same architecture!\"\n",
    "\n",
    "# Extract weights from both models\n",
    "weights1 = model1.get_weights()\n",
    "weights2 = model2.get_weights()\n",
    "weights3 = model3.get_weights()\n",
    "weights4 = model4.get_weights()\n",
    "\n",
    "# Initialize a new model by copying the architecture and weights from model1\n",
    "merged_model = tf.keras.models.clone_model(model1)\n",
    "merged_model.set_weights(weights1)\n",
    "\n",
    "# Accuracies of the models\n",
    "acc1, acc2, acc3, acc4 = 0.66, 0.87, 0.82, 0.83\n",
    "\n",
    "# Normalize the accuracies to get weights\n",
    "total_acc = acc1 + acc2 + acc3 + acc4\n",
    "wx1 = acc1 / total_acc\n",
    "wx2 = acc2 / total_acc\n",
    "wx3 = acc3 / total_acc\n",
    "wx4 = acc4 / total_acc\n",
    "\n",
    "average_weights = [((w1*wx1) + (w2*wx2) + (w3*wx3) + (w4*wx4)) / n for w1, w2, w3, w4 in zip(weights1, weights2, weights3, weights4)]\n",
    "merged_model.set_weights(average_weights)\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save('merged_model.h5')\n",
    "\n",
    "print(\"Merged model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the dataset for testing\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    r\"split_chest_xray\\test\",  # Replace with the path to your testing dataset directory\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model3 = tf.keras.models.load_model('merged_model.h5')\n",
    "model3.compile(optimizer='adam',  # or your preferred optimizer\n",
    "               loss='categorical_crossentropy',  # or your preferred loss function\n",
    "               metrics=['accuracy'])  # or your preferred metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.8923 - loss: 0.3255\n",
      "Test Loss: 0.3353727459907532, Test Accuracy: 0.8910256624221802\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the testing data\n",
    "test_loss, test_accuracy = model3.evaluate(test_generator)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
